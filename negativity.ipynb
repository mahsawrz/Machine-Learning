{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUfdV_vIIrA1",
        "outputId": "cc6768c2-9ef4-4d0c-9d84-7665f39496d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression Model Accuracy: 35.71% \n",
            "\n",
            "Mean squared error of the LogisticRegression model: 30.36% \n",
            "\n",
            "Mean absolute error of the LogisticRegression model: 3.93% \n",
            "\n",
            "Recall of the LogisticRegression model: 35.71% \n",
            "\n",
            "Precision of the LogisticRegression model: 22.62% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('Manfigera.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:,0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a logistic regression classifier as the final step.\n",
        "\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "\n",
        "# Training and Evaluation: Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#fits the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# makes predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#calculates the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"LogisticRegression Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the LogisticRegression model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the LogisticRegression model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Recall of the LogisticRegression model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Precision of the LogisticRegression model: {:.2f}%\".format(precision),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('Manfigera.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:,0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a Naive Bayes classifier as the final step.\n",
        "\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "\n",
        "# Training and Evaluation: Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#fits the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# makes predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#calculates the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Naive Bayes Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Naive Bayes model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the Naive Bayes model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Recall of the Naive Bayes model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Precision of the Naive Bayes model: {:.2f}%\".format(precision),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTO9wl9dJGrY",
        "outputId": "385d0167-a968-4da1-c613-f3d07f065d80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy: 64.29% \n",
            "\n",
            "Mean squared error of the Naive Bayes model: 8.93% \n",
            "\n",
            "Mean absolute error of the Naive Bayes model: 1.79% \n",
            "\n",
            "Recall of the Naive Bayes model: 64.29% \n",
            "\n",
            "Precision of the Naive Bayes model: 67.62% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('Manfigera.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:,0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a SVR classifier as the final step.\n",
        "\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVR())\n",
        "])\n",
        "\n",
        "\n",
        "# Training and Evaluation: Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#fits the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# makes predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Calculate and display the accuracy of the model (using Mean Squared Error as an example)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "print(\"Accuracy of the SVR model: {:.2f}%\".format(accuracy * 100),\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the SVR model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the SVR model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Recall of the SVR model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Precision of the SVR model: {:.2f}%\".format(precision),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J020tP59JMT0",
        "outputId": "a68312ed-49f8-492a-f461-4f21b0dc4386"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVR model: 52.29% \n",
            "\n",
            "Mean squared error of the SVR model: 22.81% \n",
            "\n",
            "Mean absolute error of the SVR model: 3.96% \n",
            "\n",
            "Recall of the SVR model: 14.29% \n",
            "\n",
            "Precision of the SVR model: 14.29% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, recall_score, precision_score, r2_score\n",
        "import sys\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('Manfigera.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:, 1:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a linear regression classifier as the final step.\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate and display the R-squared score\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r_squared = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Accuracy score of the Polynomial model: {:.2f}%\".format(sys.getsizeof(r_squared)), \"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Polynomial model: {:.2f}%\".format(mse), \"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the Polynomial model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Recall of the Polynomial model: {:.2f}%\".format(recall), \"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Precision of the Polynomial model: {:.2f}%\".format(precision), \"\\n\")"
      ],
      "metadata": {
        "id": "cqQ8yhe_JhmD",
        "outputId": "2145fed4-bdf2-4e5f-8ed5-ea49dc8602ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score of the Polynomial model: 32.00% \n",
            "\n",
            "Mean squared error of the Polynomial model: 59.84% \n",
            "\n",
            "Mean absolute error of the Polynomial model: 6.64% \n",
            "\n",
            "Recall of the Polynomial model: 7.14% \n",
            "\n",
            "Precision of the Polynomial model: 10.71% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "\n",
        "# Data Loading and Separation\n",
        "data = pd.read_excel('Manfigera.xlsx')\n",
        "X = data.iloc[:, 1:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]   # Assuming the output is in the first column\n",
        "\n",
        "# Data Preprocessing Setup\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier())  # Use Decision Tree instead of Logistic Regression\n",
        "])\n",
        "\n",
        "# Training and Evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Metrics Calculation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "precision = precision_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "\n",
        "# Display Metrics\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Decision Tree model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "print(\"Mean absolute error of the Decision Tree model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "print(\"Recall of the Decision Tree model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "print(\"Precision of the Decision Tree model: {:.2f}%\".format(precision),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40h--uOI-0qk",
        "outputId": "1a6af671-0235-4953-b98f-d57f08cc5bcc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Model Accuracy: 21.43% \n",
            "\n",
            "Mean squared error of the Decision Tree model: 91.07% \n",
            "\n",
            "Mean absolute error of the Decision Tree model: 7.50% \n",
            "\n",
            "Recall of the Decision Tree model: 21.43% \n",
            "\n",
            "Precision of the Decision Tree model: 21.09% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}