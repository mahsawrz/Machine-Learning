{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Khodshifte.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the third column to the last)\n",
        "features = df.iloc[:, 0:]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "features = features.fillna(features.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in features.select_dtypes(include=['object']).columns:\n",
        "    features[column] = label_encoder.fit_transform(features[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "normalized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
        "\n",
        "# Extract the output column (assuming it is the first column)\n",
        "output_column = df.iloc[:, 0]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(normalized_features, output_column, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a logistic regression model\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = logreg_model.predict(X_test)\n",
        "\n",
        "# Calculate and display the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "\n",
        "print(\"Accuracy of the Logistic Regression model: {:.2f}%\".format(accuracy * 100),\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the LogisticRegression model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the LogisticRegression model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Recall of the LogisticRegression model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Precision of the LogisticRegression model: {:.2f}%\".format(precision),\"\\n\")"
      ],
      "metadata": {
        "id": "u7Mr2MrcJ_a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369b27a6-d943-4ca6-a2bf-07ce9192258b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Logistic Regression model: 71.43% \n",
            "\n",
            "Mean squared error of the LogisticRegression model: 7.14% \n",
            "\n",
            "Mean absolute error of the LogisticRegression model: 1.43% \n",
            "\n",
            "Recall of the LogisticRegression model: 71.43% \n",
            "\n",
            "Precision of the LogisticRegression model: 70.24% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8812f3cc8d52>:18: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  features = features.fillna(features.mean())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = \"Khodshifte.xlsx\"  # Replace with your file path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate input features (from the second to the last column) and output variable (first column)\n",
        "X = data.iloc[:, 0:]\n",
        "y = data.iloc[:, 0]\n",
        "\n",
        "# Step 1: Separate numeric and non-numeric columns\n",
        "numeric_cols = X.select_dtypes(include=['number']).columns\n",
        "non_numeric_cols = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Step 2: Handle missing values using SimpleImputer with different strategies for numeric and non-numeric columns\n",
        "imputer_numeric = SimpleImputer(strategy='mean')\n",
        "imputer_non_numeric = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_numeric = pd.DataFrame(imputer_numeric.fit_transform(X[numeric_cols]), columns=numeric_cols)\n",
        "X_non_numeric = pd.DataFrame(imputer_non_numeric.fit_transform(X[non_numeric_cols]), columns=non_numeric_cols)\n",
        "\n",
        "# Combine the numeric and non-numeric columns\n",
        "X = pd.concat([X_numeric, X_non_numeric], axis=1)\n",
        "\n",
        "# Step 3: Normalize input features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 4: Encode non-numeric data using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in non_numeric_cols:\n",
        "    X[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "# Step 5: Create a Naive Bayes model (Gaussian Naive Bayes for continuous features)\n",
        "model = Pipeline([\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "# Step 6: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Display the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test,  y_pred)\n",
        "\n",
        "print(f\"Naive Bayes Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Naive Bayes model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the Naive Bayes model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Recall of the Naive Bayes model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred.round(), average='weighted') * 100\n",
        "print(\"Precision of the Naive Bayes model: {:.2f}%\".format(precision),\"\\n\")"
      ],
      "metadata": {
        "id": "INbYjy9dKDPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb29b2fe-28c1-46e6-f48e-5604b9c42cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy: 85.71% \n",
            "\n",
            "Mean squared error of the Naive Bayes model: 8.93% \n",
            "\n",
            "Mean absolute error of the Naive Bayes model: 1.07% \n",
            "\n",
            "Recall of the Naive Bayes model: 85.71% \n",
            "\n",
            "Precision of the Naive Bayes model: 82.65% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Khodshifte.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the second column to the last)\n",
        "X = df.iloc[:, 0:]\n",
        "\n",
        "# Extract the output column (assuming it is the first column)\n",
        "y = df.iloc[:, 0]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    X[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVR model\n",
        "svr_model = SVR()\n",
        "\n",
        "# Train the model on the training set\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = svr_model.predict(X_test)\n",
        "\n",
        "# Calculate and display the accuracy of the model (using Mean Squared Error as an example)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "print(\"Accuracy of the SVR model: {:.2f}%\".format(accuracy * 100),\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the SVR model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the SVR model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Recall of the SVR model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Precision of the SVR model: {:.2f}%\".format(precision),\"\\n\")"
      ],
      "metadata": {
        "id": "Q8DDfYTFKG_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0074e254-cc87-4d2a-8e35-3a5d00e3d1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVR model: 41.03% \n",
            "\n",
            "Mean squared error of the SVR model: 13.28% \n",
            "\n",
            "Mean absolute error of the SVR model: 2.67% \n",
            "\n",
            "Recall of the SVR model: 14.29% \n",
            "\n",
            "Precision of the SVR model: 42.86% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-57c5b0fb7826>:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  X = X.fillna(X.mean())\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sys\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Khodshifte.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the second column to the last)\n",
        "X = df.iloc[:, 0:]  # Assuming the input features start from the second column\n",
        "y = df.iloc[:, 0]   # Assuming the output is in the first column\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables using OneHotEncoder\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 3: Apply Polynomial Regression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Create a pipeline with preprocessing and Polynomial Regression\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('poly', poly),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate and display the accuracy of the model (using Mean Squared Error as an example)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "print(\"Accuracy of the Polynomial Regression model: {:.2f}%\".format(sys.getsizeof(accuracy)), \"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Polynomial Regression model: {:.2f}%\".format(mse), \"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the Polynomial Regression model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "# Convert predictions to binary values based on the threshold\n",
        "threshold = 3\n",
        "binary_predictions = (predictions > threshold).astype(int)\n",
        "\n",
        "# Ensure that there are positive predictions to calculate precision and recall\n",
        "if 1 in binary_predictions:\n",
        "    # Calculate and display precision and recall\n",
        "    precision = precision_score(y_test, binary_predictions, average='micro')  # You can use 'macro' or 'weighted' as well\n",
        "    recall = recall_score(y_test, binary_predictions, average='micro')\n",
        "\n",
        "    print(\"Precision of the Polynomial Regression model: {:.2f}%\".format(precision * 100), \"\\n\")\n",
        "    print(\"Recall of the Polynomial Regression model: {:.2f}%\".format(recall* 100))\n",
        "else:\n",
        "    print(\"No positive predictions. Adjust the threshold or check the data.\")"
      ],
      "metadata": {
        "id": "UHz28HMWKVC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a33888-c031-49ee-9b86-ee8efbe9e51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Polynomial Regression model: 32.00% \n",
            "\n",
            "Mean squared error of the Polynomial Regression model: 3.89% \n",
            "\n",
            "Mean absolute error of the Polynomial Regression model: 1.66% \n",
            "\n",
            "Precision of the Polynomial Regression model: 42.86% \n",
            "\n",
            "Recall of the Polynomial Regression model: 42.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-33e4f677cc47>:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  X = X.fillna(X.mean())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Khodshifte.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the third column to the last)\n",
        "features = df.iloc[:, 1:]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "features = features.fillna(features.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in features.select_dtypes(include=['object']).columns:\n",
        "    features[column] = label_encoder.fit_transform(features[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "normalized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
        "\n",
        "# Extract the output column (assuming it is the first column)\n",
        "output_column = df.iloc[:, 0]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(normalized_features, output_column, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree model\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = decision_tree_model.predict(X_test)\n",
        "\n",
        "# Calculate and display the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "\n",
        "print(\"Accuracy of the Decision Tree model: {:.2f}%\".format(accuracy * 100), \"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Decision Tree model: {:.2f}%\".format(mse), \"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the Decision Tree model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, predictions, average='weighted') * 100\n",
        "print(\"Recall of the Decision Tree model: {:.2f}%\".format(recall), \"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='weighted') * 100\n",
        "print(\"Precision of the Decision Tree model: {:.2f}%\".format(precision), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn3wosnPI_iw",
        "outputId": "0f3838f9-b9e8-44c6-a126-863cfde15681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree model: 28.57% \n",
            "\n",
            "Mean squared error of the Decision Tree model: 42.86% \n",
            "\n",
            "Mean absolute error of the Decision Tree model: 5.00% \n",
            "\n",
            "Recall of the Decision Tree model: 28.57% \n",
            "\n",
            "Precision of the Decision Tree model: 17.14% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-a56eaf5e2535>:15: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  features = features.fillna(features.mean())\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}