{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RChcz8dN22oT",
        "outputId": "f12f928d-f8a7-400f-8ab9-98f6a14d1372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression Model Accuracy: 64.29% \n",
            "\n",
            "Mean squared error of the LogisticRegression model: 19.64% \n",
            "\n",
            "Mean absolute error of the LogisticRegression model: 2.50% \n",
            "\n",
            "Recall of the LogisticRegression model: 64.29% \n",
            "\n",
            "Precision of the LogisticRegression model: 46.03% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('OmideBzendegi.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:, 0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a logistic regression classifier as the final step.\n",
        "\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "\n",
        "# Training and Evaluation: Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#fits the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# makes predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#calculates the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"LogisticRegression Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the LogisticRegression model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the LogisticRegression model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Recall of the LogisticRegression model: {:.2f}%\".format(recall), \"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Precision of the LogisticRegression model: {:.2f}%\".format(precision), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('OmideBzendegi.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:,0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a NB classifier as the final step.\n",
        "\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "\n",
        "# Training and Evaluation: Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#fits the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# makes predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#calculates the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Naive Bayes Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Naive Bayes model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the Naive Bayes model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Recall of the Naive Bayes model: {:.2f}%\".format(recall), \"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Precision of the Naive Bayes model: {:.2f}%\".format(precision), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBL5wHvw5pL4",
        "outputId": "cf590433-1fc8-4e30-ff48-80d5f06ee86e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy: 71.43% \n",
            "\n",
            "Mean squared error of the Naive Bayes model: 12.50% \n",
            "\n",
            "Mean absolute error of the Naive Bayes model: 1.79% \n",
            "\n",
            "Recall of the Naive Bayes model: 71.43% \n",
            "\n",
            "Precision of the Naive Bayes model: 71.43% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Data Loading and Separation\n",
        "data = pd.read_excel('OmideBzendegi.xlsx')\n",
        "X = data.iloc[:, 0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]   # Assuming the output is in the first column\n",
        "\n",
        "# Data Preprocessing Setup\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVR())\n",
        "])\n",
        "\n",
        "# Training and Evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and display metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred.round(), average='macro')  # Use 'macro' for multiple classes\n",
        "precision = precision_score(y_test, y_pred.round(), average='macro')  # Use 'macro' for multiple classes\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "\n",
        "print(f\"SVR Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "print(\"Mean squared error of the SVR model: {:.2f}%\".format(mse), \"\\n\")\n",
        "print(\"Mean absolute error of the SVR model: {:.2f}%\".format(mae), \"\\n\")\n",
        "print(\"Recall of the SVR model: {:.2f}%\".format(recall*100 ), \"\\n\")\n",
        "print(\"Precision of the SVR model: {:.2f}%\".format(precision*100), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtjfT39N6FzF",
        "outputId": "5d14ba73-c70a-4a36-80c2-2dc789c78057"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR Model Accuracy: 31.93% \n",
            "\n",
            "Mean squared error of the SVR model: 33.01% \n",
            "\n",
            "Mean absolute error of the SVR model: 3.84% \n",
            "\n",
            "Recall of the SVR model: 7.14% \n",
            "\n",
            "Precision of the SVR model: 10.00% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('OmideBzendegi.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:, 0:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a linear regression classifier as the final step.\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('classifier', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate and display the metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions.round(), average='weighted')\n",
        "precision = precision_score(y_test, predictions.round(), average='weighted')\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "\n",
        "print(\"Accuracy of the Polynomial model: {:.2f}%\".format(accuracy * 100), \"\\n\")\n",
        "print(\"Mean squared error of the Polynomial model: {:.2f}%\".format(mse), \"\\n\")\n",
        "print(\"Mean absolute error of the Polynomial model: {:.2f}%\".format(mae), \"\\n\")\n",
        "print(\"Recall of the Polynomial model: {:.2f}%\".format(recall * 100), \"\\n\")\n",
        "print(\"Precision of the Polynomial model: {:.2f}%\".format(precision * 100), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9V6quumORay",
        "outputId": "fd96ea5f-d88f-4480-d96c-6f5c409bb3b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Polynomial model: 67.16% \n",
            "\n",
            "Mean squared error of the Polynomial model: 15.93% \n",
            "\n",
            "Mean absolute error of the Polynomial model: 1.78% \n",
            "\n",
            "Recall of the Polynomial model: 50.00% \n",
            "\n",
            "Precision of the Polynomial model: 64.29% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Data Loading and Separation: Reads an Excel file into a pandas DataFrame (data)\n",
        "data = pd.read_excel('OmideBzendegi.xlsx')\n",
        "\n",
        "# Separate input features and target variable\n",
        "X = data.iloc[:, 1:]  # Assuming the input features start from the second column\n",
        "y = data.iloc[:, 0]  # Assuming the output is in the first column\n",
        "\n",
        "# Data Preprocessing Setup: Converts the 'lastPost-date' column to a datetime format.\n",
        "# Identifies different types of features: numeric, datetime, and non-numeric (object) features.\n",
        "X['lastPost-date'] = pd.to_datetime(X['lastPost-date'], errors='coerce')\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "date_features = X.select_dtypes(include=['datetime']).columns\n",
        "non_numeric_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Preprocessing Pipeline Setup: Defines separate pipelines for numeric and non-numeric (categorical) features.\n",
        "# Utilizes SimpleImputer for missing values and StandardScaler for numeric features, while using SimpleImputer and\n",
        "# OneHotEncoder for non-numeric features. These are combined using ColumnTransformer.\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "non_numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('non_num', non_numeric_transformer, non_numeric_features)\n",
        "])\n",
        "\n",
        "# Model Creation: Constructs a pipeline for the overall process.\n",
        "# It includes the preprocessing steps defined earlier and a Decision Tree classifier as the final step.\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "# Training and Evaluation: Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fits the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Makes predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculates and displays the metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Decision Tree model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the Decision Tree model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Recall of the Decision Tree model: {:.2f}%\".format(recall), \"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Precision of the Decision Tree model: {:.2f}%\".format(precision), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpxZFSWdPJFZ",
        "outputId": "6901e1dc-5656-44bc-b557-cce95f4e4d36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Model Accuracy: 28.57% \n",
            "\n",
            "Mean squared error of the Decision Tree model: 57.14% \n",
            "\n",
            "Mean absolute error of the Decision Tree model: 5.71% \n",
            "\n",
            "Recall of the Decision Tree model: 28.57% \n",
            "\n",
            "Precision of the Decision Tree model: 28.57% \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}