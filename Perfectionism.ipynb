{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH9_3htrwJSD",
        "outputId": "d3c1264f-7c15-462b-d854-045ca83c990f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the Logistic Regression model: 57.14% \n",
            "\n",
            "Mean squared error of the Logistic Regression model: 35.71% \n",
            "\n",
            "Mean absolute error of the Logistic Regression model: 3.57% \n",
            "\n",
            "Recall of the Logistic Regression model: 57.14% \n",
            "\n",
            "Precision of the Logistic Regression model: 51.19% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-37396ff8af5c>:15: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  features = features.fillna(features.mean())\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Kamalgera.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the third column to the last)\n",
        "features = df.iloc[:, 0:]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "features = features.fillna(features.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in features.select_dtypes(include=['object']).columns:\n",
        "    features[column] = label_encoder.fit_transform(features[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "normalized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
        "\n",
        "# Extract the output column (assuming it is the first column)\n",
        "output_column = df.iloc[:, 0]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(normalized_features, output_column, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a logistic regression model\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = logreg_model.predict(X_test)\n",
        "\n",
        "# Calculate and display the metrics as percentages\n",
        "accuracy = accuracy_score(y_test, predictions) * 100\n",
        "print(\"Accuracy of the Logistic Regression model: {:.2f}%\".format(accuracy), \"\\n\")\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"Mean squared error of the Logistic Regression model: {:.2f}%\".format(mse), \"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the Logistic Regression model: {:.2f}%\".format(mae), \"\\n\")\n",
        "\n",
        "# Update with average='weighted' for multiclass targets\n",
        "recall = recall_score(y_test, predictions, average='weighted') * 100\n",
        "print(\"Recall of the Logistic Regression model: {:.2f}%\".format(recall), \"\\n\")\n",
        "\n",
        "# Update with average='weighted' for multiclass targets\n",
        "precision = precision_score(y_test, predictions, average='weighted') * 100\n",
        "print(\"Precision of the Logistic Regression model: {:.2f}%\".format(precision), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-pbHEhkgLO",
        "outputId": "a482a6a6-9c6a-4896-bb9d-b6c1326f45e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Model Accuracy: 78.57% \n",
            "\n",
            "Mean squared error of the Naive Bayes model: 5.36% \n",
            "\n",
            "Mean absolute error of the Naive Bayes model: 1.07% \n",
            "\n",
            "Recall of the Naive Bayes model: 78.57% \n",
            "\n",
            "Precision of the Naive Bayes model: 79.76% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = \"Kamalgera.xlsx\"  # Replace with your file path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Separate input features (from the second to the last column) and output variable (first column)\n",
        "X = data.iloc[:, 0:]\n",
        "y = data.iloc[:, 0]\n",
        "\n",
        "# Step 1: Separate numeric and non-numeric columns\n",
        "numeric_cols = X.select_dtypes(include=['number']).columns\n",
        "non_numeric_cols = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Step 2: Handle missing values using SimpleImputer with different strategies for numeric and non-numeric columns\n",
        "imputer_numeric = SimpleImputer(strategy='mean')\n",
        "imputer_non_numeric = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_numeric = pd.DataFrame(imputer_numeric.fit_transform(X[numeric_cols]), columns=numeric_cols)\n",
        "X_non_numeric = pd.DataFrame(imputer_non_numeric.fit_transform(X[non_numeric_cols]), columns=non_numeric_cols)\n",
        "\n",
        "# Combine the numeric and non-numeric columns\n",
        "X = pd.concat([X_numeric, X_non_numeric], axis=1)\n",
        "\n",
        "# Step 3: Normalize input features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 4: Encode non-numeric data using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in non_numeric_cols:\n",
        "    X[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "# Step 5: Create a Naive Bayes model (Gaussian Naive Bayes for continuous features)\n",
        "model = Pipeline([\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "# Step 6: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Display the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Naive Bayes Model Accuracy: {accuracy * 100:.2f}%\",\"\\n\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error of the Naive Bayes model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error of the Naive Bayes model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Recall of the Naive Bayes model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
        "print(\"Precision of the Naive Bayes model: {:.2f}%\".format(precision),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DgdnMMflH0x",
        "outputId": "1f3d337a-5e70-4266-b174-47e391e95dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVR model: 30.94% \n",
            "\n",
            "Mean squared error of the SVR model: 62.23% \n",
            "\n",
            "Mean absolute error of the SVR model: 6.54% \n",
            "\n",
            "Recall of the SVR model: 7.14% \n",
            "\n",
            "Precision of the SVR model: 1.79% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-c6e58bf90ae8>:19: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  X = X.fillna(X.mean())\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Kamalgera.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the second column to the last)\n",
        "X = df.iloc[:, 0:]\n",
        "\n",
        "# Extract the output column (assuming it is the first column)\n",
        "y = df.iloc[:, 0]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    X[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVR model\n",
        "svr_model = SVR()\n",
        "\n",
        "# Train the model on the training set\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = svr_model.predict(X_test)\n",
        "\n",
        "# Calculate and display the accuracy of the model (using Mean Squared Error as an example)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "print(\"Accuracy of the SVR model: {:.2f}%\".format(accuracy * 100),\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the SVR model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the SVR model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Recall of the SVR model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Precision of the SVR model: {:.2f}%\".format(precision),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bTVT26JEaU0",
        "outputId": "67fb72c5-b989-40df-98d8-25b87876df43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the Polynomial model: 48.36% \n",
            "\n",
            "Mean squared error of the Polynomial model: 46.53% \n",
            "\n",
            "Mean absolute error of the Polynomial model: 4.65% \n",
            "\n",
            "Recall of the Polynomial model: 7.14% \n",
            "\n",
            "Precision of the Polynomial model: 14.29% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-36-ad2875666783>:21: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  X = X.fillna(X.mean())\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Kamalgera.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the second to last column)\n",
        "X = df.iloc[:, 0:]\n",
        "\n",
        "# Extract the output column (first column)\n",
        "y = df.iloc[:, 0]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    X[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "normalized_features = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(normalized_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Polynomial Regression\n",
        "degree = 2  # You can adjust the degree based on your needs\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "poly_model = LinearRegression()\n",
        "\n",
        "# Train the model on the polynomial features\n",
        "poly_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = poly_model.predict(X_test_poly)\n",
        "\n",
        "# Calculate and display the R-squared score\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "accuracy = 1 - mse / y_test.var()\n",
        "\n",
        "print(\"Accuracy of the Polynomial model: {:.2f}%\".format(accuracy * 100),\"\\n\")\n",
        "\n",
        "print(\"Mean squared error of the Polynomial model: {:.2f}%\".format(mse),\"\\n\")\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean absolute error of the Polynomial model: {:.2f}%\".format(mae),\"\\n\")\n",
        "\n",
        "recall = recall_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Recall of the Polynomial model: {:.2f}%\".format(recall),\"\\n\")\n",
        "\n",
        "precision = precision_score(y_test, predictions.round(), average='weighted') * 100\n",
        "print(\"Precision of the Polynomial model: {:.2f}%\".format(precision),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR6McbWC3OGT",
        "outputId": "6707f889-0c8f-4d27-85be-b0e89f76a53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the Decision Tree model: 21.43% \n",
            "\n",
            "Mean squared error of the Decision Tree model: 67.86% \n",
            "\n",
            "Mean absolute error of the Decision Tree model: 6.43% \n",
            "\n",
            "Recall of the Decision Tree model: 21.43% \n",
            "\n",
            "Precision of the Decision Tree model: 26.19% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-52-7fff978b5439>:16: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  features = features.fillna(features.mean())\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"Kamalgera.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the features (from the third column to the last)\n",
        "features = df.iloc[:, 1:]  # Assuming the first column is the target variable\n",
        "output_column = df.iloc[:, 0]\n",
        "\n",
        "# Step 1: Handle missing values (if any)\n",
        "features = features.fillna(features.mean())\n",
        "\n",
        "# Step 2: Encode categorical variables (if any)\n",
        "label_encoder = LabelEncoder()\n",
        "for column in features.select_dtypes(include=['object']).columns:\n",
        "    features[column] = label_encoder.fit_transform(features[column])\n",
        "\n",
        "# Step 3: Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "normalized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(normalized_features, output_column, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a decision tree model\n",
        "dt_model = DecisionTreeClassifier()\n",
        "\n",
        "# Train the decision tree model on the training set\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set using decision tree\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate decision tree model\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions) * 100\n",
        "print(\"Accuracy of the Decision Tree model: {:.2f}%\".format(dt_accuracy), \"\\n\")\n",
        "\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "print(\"Mean squared error of the Decision Tree model: {:.2f}%\".format(dt_mse), \"\\n\")\n",
        "\n",
        "dt_mae = mean_absolute_error(y_test, dt_predictions)\n",
        "print(\"Mean absolute error of the Decision Tree model: {:.2f}%\".format(dt_mae), \"\\n\")\n",
        "\n",
        "dt_recall = recall_score(y_test, dt_predictions, average='weighted') * 100\n",
        "print(\"Recall of the Decision Tree model: {:.2f}%\".format(dt_recall), \"\\n\")\n",
        "\n",
        "dt_precision = precision_score(y_test, dt_predictions, average='weighted') * 100\n",
        "print(\"Precision of the Decision Tree model: {:.2f}%\".format(dt_precision), \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
