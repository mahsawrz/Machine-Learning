# -*- coding: utf-8 -*-
"""Khodshifte.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NF1VqKsHwKfkxbiCPOWV9Pbx-qSBJPxZ
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score


# Load the dataset
file_path = "Khodshifte.xlsx"
df = pd.read_excel(file_path)

# Extract the features (from the third column to the last)
features = df.iloc[:, 0:]

# Step 1: Handle missing values (if any)
features = features.fillna(features.mean())

# Step 2: Encode categorical variables (if any)
label_encoder = LabelEncoder()
for column in features.select_dtypes(include=['object']).columns:
    features[column] = label_encoder.fit_transform(features[column])

# Step 3: Normalize numerical features
scaler = StandardScaler()
normalized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)

# Extract the output column (assuming it is the first column)
output_column = df.iloc[:, 0]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(normalized_features, output_column, test_size=0.2, random_state=42)

# Create a logistic regression model
logreg_model = LogisticRegression()

# Train the model on the training set
logreg_model.fit(X_train, y_train)

# Make predictions on the testing set
predictions = logreg_model.predict(X_test)

# Calculate and display the accuracy of the model
accuracy = accuracy_score(y_test, predictions)
mse = mean_squared_error(y_test, predictions)

print("Accuracy of the Logistic Regression model: {:.2f}%".format(accuracy * 100),"\n")

print("Mean squared error of the LogisticRegression model: {:.2f}%".format(mse),"\n")

mae = mean_absolute_error(y_test, predictions)
print("Mean absolute error of the LogisticRegression model: {:.2f}%".format(mae),"\n")

recall = recall_score(y_test, predictions.round(), average='weighted') * 100
print("Recall of the LogisticRegression model: {:.2f}%".format(recall),"\n")

precision = precision_score(y_test, predictions.round(), average='weighted') * 100
print("Precision of the LogisticRegression model: {:.2f}%".format(precision),"\n")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score

# Load the Excel file
file_path = "Khodshifte.xlsx"  # Replace with your file path
data = pd.read_excel(file_path)

# Separate input features (from the second to the last column) and output variable (first column)
X = data.iloc[:, 0:]
y = data.iloc[:, 0]

# Step 1: Separate numeric and non-numeric columns
numeric_cols = X.select_dtypes(include=['number']).columns
non_numeric_cols = X.select_dtypes(exclude=['number']).columns

# Step 2: Handle missing values using SimpleImputer with different strategies for numeric and non-numeric columns
imputer_numeric = SimpleImputer(strategy='mean')
imputer_non_numeric = SimpleImputer(strategy='most_frequent')

X_numeric = pd.DataFrame(imputer_numeric.fit_transform(X[numeric_cols]), columns=numeric_cols)
X_non_numeric = pd.DataFrame(imputer_non_numeric.fit_transform(X[non_numeric_cols]), columns=non_numeric_cols)

# Combine the numeric and non-numeric columns
X = pd.concat([X_numeric, X_non_numeric], axis=1)

# Step 3: Normalize input features using StandardScaler
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# Step 4: Encode non-numeric data using LabelEncoder
label_encoder = LabelEncoder()

for column in non_numeric_cols:
    X[column] = label_encoder.fit_transform(X[column])

# Step 5: Create a Naive Bayes model (Gaussian Naive Bayes for continuous features)
model = Pipeline([
    ('classifier', GaussianNB())
])

# Step 6: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Train the model
model.fit(X_train, y_train)

# Step 8: Make predictions on the test set
y_pred = model.predict(X_test)

# Step 9: Display the accuracy
accuracy = accuracy_score(y_test, y_pred)
mse = mean_squared_error(y_test,  y_pred)

print(f"Naive Bayes Model Accuracy: {accuracy * 100:.2f}%","\n")

print("Mean squared error of the Naive Bayes model: {:.2f}%".format(mse),"\n")

mae = mean_absolute_error(y_test, y_pred)
print("Mean absolute error of the Naive Bayes model: {:.2f}%".format(mae),"\n")

recall = recall_score(y_test, y_pred.round(), average='weighted') * 100
print("Recall of the Naive Bayes model: {:.2f}%".format(recall),"\n")

precision = precision_score(y_test, y_pred.round(), average='weighted') * 100
print("Precision of the Naive Bayes model: {:.2f}%".format(precision),"\n")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score

# Load the dataset
file_path = "Khodshifte.xlsx"
df = pd.read_excel(file_path)

# Extract the features (from the second column to the last)
X = df.iloc[:, 0:]

# Extract the output column (assuming it is the first column)
y = df.iloc[:, 0]

# Step 1: Handle missing values (if any)
X = X.fillna(X.mean())

# Step 2: Encode categorical variables (if any)
label_encoder = LabelEncoder()

for column in X.select_dtypes(include=['object']).columns:
    X[column] = label_encoder.fit_transform(X[column])

# Step 3: Normalize numerical features
scaler = StandardScaler()
X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)

# Create an SVR model
svr_model = SVR()

# Train the model on the training set
svr_model.fit(X_train, y_train)

# Make predictions on the testing set
predictions = svr_model.predict(X_test)

# Calculate and display the accuracy of the model (using Mean Squared Error as an example)
mse = mean_squared_error(y_test, predictions)
accuracy = 1 - mse / y_test.var()
print("Accuracy of the SVR model: {:.2f}%".format(accuracy * 100),"\n")

print("Mean squared error of the SVR model: {:.2f}%".format(mse),"\n")

mae = mean_absolute_error(y_test, predictions)
print("Mean absolute error of the SVR model: {:.2f}%".format(mae),"\n")

recall = recall_score(y_test, predictions.round(), average='weighted') * 100
print("Recall of the SVR model: {:.2f}%".format(recall),"\n")

precision = precision_score(y_test, predictions.round(), average='weighted') * 100
print("Precision of the SVR model: {:.2f}%".format(precision),"\n")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import sys

# Load the dataset
file_path = "Khodshifte.xlsx"
df = pd.read_excel(file_path)

# Extract the features (from the second column to the last)
X = df.iloc[:, 0:]  # Assuming the input features start from the second column
y = df.iloc[:, 0]   # Assuming the output is in the first column

# Step 1: Handle missing values (if any)
X = X.fillna(X.mean())

# Step 2: Encode categorical variables using OneHotEncoder
categorical_features = X.select_dtypes(include=['object']).columns
numeric_features = X.select_dtypes(include=['float64', 'int64']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Step 3: Apply Polynomial Regression
poly = PolynomialFeatures(degree=2)

# Create a pipeline with preprocessing and Polynomial Regression
model = Pipeline([
    ('preprocessor', preprocessor),
    ('poly', poly),
    ('regressor', LinearRegression())
])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model on the training set
model.fit(X_train, y_train)

# Make predictions on the testing set
predictions = model.predict(X_test)

# Calculate and display the accuracy of the model (using Mean Squared Error as an example)
mse = mean_squared_error(y_test, predictions)
accuracy = 1 - mse / y_test.var()
print("Accuracy of the Polynomial Regression model: {:.2f}%".format(sys.getsizeof(accuracy)), "\n")

print("Mean squared error of the Polynomial Regression model: {:.2f}%".format(mse), "\n")

mae = mean_absolute_error(y_test, predictions)
print("Mean absolute error of the Polynomial Regression model: {:.2f}%".format(mae), "\n")

# Convert predictions to binary values based on the threshold
threshold = 3
binary_predictions = (predictions > threshold).astype(int)

# Ensure that there are positive predictions to calculate precision and recall
if 1 in binary_predictions:
    # Calculate and display precision and recall
    precision = precision_score(y_test, binary_predictions, average='micro')  # You can use 'macro' or 'weighted' as well
    recall = recall_score(y_test, binary_predictions, average='micro')

    print("Precision of the Polynomial Regression model: {:.2f}%".format(precision * 100), "\n")
    print("Recall of the Polynomial Regression model: {:.2f}%".format(recall* 100))
else:
    print("No positive predictions. Adjust the threshold or check the data.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, recall_score, precision_score

# Load the dataset
file_path = "Khodshifte.xlsx"
df = pd.read_excel(file_path)

# Extract the features (from the third column to the last)
features = df.iloc[:, 1:]

# Step 1: Handle missing values (if any)
features = features.fillna(features.mean())

# Step 2: Encode categorical variables (if any)
label_encoder = LabelEncoder()
for column in features.select_dtypes(include=['object']).columns:
    features[column] = label_encoder.fit_transform(features[column])

# Step 3: Normalize numerical features
scaler = StandardScaler()
normalized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)

# Extract the output column (assuming it is the first column)
output_column = df.iloc[:, 0]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(normalized_features, output_column, test_size=0.2, random_state=42)

# Create a Decision Tree model
decision_tree_model = DecisionTreeClassifier(random_state=42)

# Train the model on the training set
decision_tree_model.fit(X_train, y_train)

# Make predictions on the testing set
predictions = decision_tree_model.predict(X_test)

# Calculate and display the accuracy of the model
accuracy = accuracy_score(y_test, predictions)
mse = mean_squared_error(y_test, predictions)

print("Accuracy of the Decision Tree model: {:.2f}%".format(accuracy * 100), "\n")

print("Mean squared error of the Decision Tree model: {:.2f}%".format(mse), "\n")

mae = mean_absolute_error(y_test, predictions)
print("Mean absolute error of the Decision Tree model: {:.2f}%".format(mae), "\n")

recall = recall_score(y_test, predictions, average='weighted') * 100
print("Recall of the Decision Tree model: {:.2f}%".format(recall), "\n")

precision = precision_score(y_test, predictions, average='weighted') * 100
print("Precision of the Decision Tree model: {:.2f}%".format(precision), "\n")

import matplotlib.pyplot as plt

# Performance criteria and their corresponding values
performance_criteria = ['accuracy_score', 'precision', 'recall', 'mae', 'mean_squared_error']
values = [71.43, 70.24, 71.43, 1.43, 7.14]

# Create a bar graph
plt.bar(performance_criteria, values, color ='purple')

# Add labels and title
plt.xlabel('Performance Criteria')
plt.ylabel('Percentage')
plt.title('Performance Evaluation of Logistic Regression')

# Display the graph
plt.show()

import matplotlib.pyplot as plt

# Performance criteria and their corresponding values
performance_criteria = ['accuracy_score', 'precision', 'recall', 'mae', 'mean_squared_error']
values = [85.71, 82.65, 85.71, 1.07, 8.93]

# Create a bar graph
plt.bar(performance_criteria, values, color ='purple')

# Add labels and title
plt.xlabel('Performance Criteria')
plt.ylabel('Percentage')
plt.title('Performance Evaluation of Naive Bayes')

# Display the graph
plt.show()

import matplotlib.pyplot as plt

# Performance criteria and their corresponding values
performance_criteria = ['accuracy_score', 'precision', 'recall', 'mae', 'mean_squared_error']
values = [41.03, 42.86, 14.29, 2.67, 13.28]

# Create a bar graph
plt.bar(performance_criteria, values, color ='purple')

# Add labels and title
plt.xlabel('Performance Criteria')
plt.ylabel('Percentage')
plt.title('Performance Evaluation of SVR')

# Display the graph
plt.show()

import matplotlib.pyplot as plt

# Performance criteria and their corresponding values
performance_criteria = ['accuracy_score', 'precision', 'recall', 'mae', 'mean_squared_error']
values = [32.00, 42.86, 42.86, 1.66, 3.89]

# Create a bar graph
plt.bar(performance_criteria, values, color ='purple')

# Add labels and title
plt.xlabel('Performance Criteria')
plt.ylabel('Percentage')
plt.title('Performance Evaluation of Polynomial Regression')

# Display the graph
plt.show()

import matplotlib.pyplot as plt

# Performance criteria and their corresponding values
performance_criteria = ['accuracy_score', 'precision', 'recall', 'mae', 'mean_squared_error']
values = [28.57, 17.14, 28.57, 5.00, 42.86]

# Create a bar graph
plt.bar(performance_criteria, values, color ='purple')

# Add labels and title
plt.xlabel('Performance Criteria')
plt.ylabel('Percentage')
plt.title('Performance Evaluation of Decision Tree')

# Display the graph
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Define your data
data = {
    'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
    'Accuracy': [57.14, 78.57, 30.94, 14.29, 26.19],
    'Precision': [51.19, 79.76, 1.79, 7.14, 21.43],
    'Recall': [57.14, 78.57, 7.14, 7.14, 6.43],
    'MAE': [3.57, 1.07, 6.54, 4.65, 67.86],
    'MSE': [35.71, 5.36, 62.23, 46.53, 19.64]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Set the dataset names as an index
df.set_index('Model', inplace=True)

# Create a bar graph
ax = df.plot(kind='bar', figsize=(12, 6), colormap='Paired')
ax.set_xlabel('Model')
ax.set_ylabel('Scores')
ax.set_title('Model Evaluation Results')

# Show the plot
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Define your data for each dataset
datasets = {
    'Kamalgera.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [57.14, 78.57, 30.94, 14.29, 26.19],
        'Precision': [51.19, 79.76, 1.79, 7.14, 21.43],
        'Recall': [57.14, 78.57, 7.14, 7.14, 6.43],
        'MAE': [3.57, 1.07, 6.54, 4.65, 67.86],
        'MSE': [35.71, 5.36, 62.23, 46.53, 19.64]
    },
    'Manfigera.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [35.71, 64.29, 52.29, 32.00, 21.43],
        'Precision': [22.62, 67.62, 14.29, 10.71, 21.09],
        'Recall': [35.71, 64.29, 14.29, 7.14, 21.43],
        'MAE': [3.93, 1.79, 3.96, 6.64, 7.50],
        'MSE': [30.36, 8.93, 22.81, 59.84, 91.07]
    },
    'Khodshifte.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [71.43, 85.71, 41.03, 32.00, 28.57],
        'Precision': [70.24, 82.65, 42.86, 42.86, 17.14],
        'Recall': [71.43, 85.71, 14.29, 42.86, 28.57],
        'MAE': [1.43, 1.07, 2.67, 1.66, 5.00],
        'MSE': [7.14, 8.93, 13.28, 3.89, 42.86]
    },
    'OmideBzendegi.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [64.29, 71.43, 31.93, 67.16, 28.57],
        'Precision': [46.03, 71.43, 10.00, 64.29, 28.57],
        'Recall': [64.29, 71.43, 7.14, 50.00, 28.57],
        'MAE': [2.50, 1.79, 3.84, 1.78, 5.71],
        'MSE': [19.64, 12.50, 33.01, 15.93, 57.14]
    },
}

# Create subplots in a 2x2 grid
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 12), sharex=True)

# Create a color map
cmap = plt.get_cmap('Paired')

# Plot each dataset in the corresponding subplot
for i, (dataset_name, data) in enumerate(datasets.items()):
    row = i // 2
    col = i % 2
    df = pd.DataFrame(data)
    df.set_index('Model', inplace=True)
    ax = df.plot(kind='bar', ax=axes[row, col], colormap=cmap)
    ax.set_ylabel('Scores')
    ax.set_title(f'Model Evaluation - {dataset_name}')

# Set common x-axis label
plt.xlabel('Model')

# Adjust layout
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Define your data for each dataset
datasets = {
    'Kamalgera.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [57.14, 78.57, 30.94, 14.29, 26.19],
        'Precision': [51.19, 79.76, 1.79, 7.14, 21.43],
        'Recall': [57.14, 78.57, 7.14, 7.14, 6.43],
        'MAE': [3.57, 1.07, 6.54, 4.65, 67.86],
        'MSE': [35.71, 5.36, 62.23, 46.53, 19.64]
    },
    'Manfigera.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [35.71, 64.29, 52.29, 32.00, 21.43],
        'Precision': [22.62, 67.62, 14.29, 10.71, 21.09],
        'Recall': [35.71, 64.29, 14.29, 7.14, 21.43],
        'MAE': [3.93, 1.79, 3.96, 6.64, 7.50],
        'MSE': [30.36, 8.93, 22.81, 59.84, 91.07]
    },
    'Khodshifte.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [71.43, 85.71, 41.03, 32.00, 28.57],
        'Precision': [70.24, 82.65, 42.86, 42.86, 17.14],
        'Recall': [71.43, 85.71, 14.29, 42.86, 28.57],
        'MAE': [1.43, 1.07, 2.67, 1.66, 5.00],
        'MSE': [7.14, 8.93, 13.28, 3.89, 42.86]
    },
    'OmideBzendegi.xlsx': {
        'Model': ['Logistic Regression', 'Naive Bayes', 'SVR', 'Polynomial Regression', 'Decision Tree'],
        'Accuracy': [64.29, 71.43, 31.93, 67.16, 28.57],
        'Precision': [46.03, 71.43, 10.00, 64.29, 28.57],
        'Recall': [64.29, 71.43, 7.14, 50.00, 28.57],
        'MAE': [2.50, 1.79, 3.84, 1.78, 5.71],
        'MSE': [19.64, 12.50, 33.01, 15.93, 57.14]
    },
}

# Create subplots in a 4x1 grid
fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(8, 12))

# Create a color map
cmap = plt.get_cmap('Paired')

# Plot each dataset in the corresponding subplot
for i, (dataset_name, data) in enumerate(datasets.items()):
    ax = axes[i]
    df = pd.DataFrame(data)
    df.set_index('Model', inplace=True)
    ax = df.plot(kind='bar', ax=ax, colormap=cmap)
    ax.set_ylabel('Scores')
    ax.set_title(f'Model Evaluation - {dataset_name}')

# Set common x-axis label
plt.xlabel('Model')

# Adjust layout
plt.tight_layout()
plt.show()